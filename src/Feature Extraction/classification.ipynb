{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import color, exposure\n",
    "from skimage.feature import hog as hog_sklearn\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "### 1. We load all the images into training and test sets and resize them to 128x128:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Taken from subsample of data:\n",
    "Max width of image is 720\n",
    "Max height of image is 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeAndPad(im, desired_size):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0])) \n",
    "\n",
    "    delta_w = desired_size - new_size[1]\n",
    "    delta_h = desired_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "        value=color)\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog(img, plot=False):\n",
    "    img_g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    fd = hog_sklearn(img_g, orientations=3,\n",
    "                        pixels_per_cell = (3, 3),\n",
    "                        cells_per_block=(3, 3),\n",
    "                        block_norm='L1', \n",
    "                        multichannel=False,\n",
    "                        feature_vector=True, \n",
    "                        visualize=plot)\n",
    "    # print(fd)\n",
    "    # print(\"fd.shape: \", fd.shape)\n",
    "    #  print(\"hog_image: \", hog_image.shape)\n",
    "    # Preview\n",
    "    if plot:\n",
    "        plt.figure(figsize = (10,10))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(\"Original Image\"), plt.xticks([]), plt.yticks([])\n",
    "        plt.show()\n",
    "        plt.imshow(hog_image)\n",
    "        plt.show()\n",
    "    return fd\n",
    "\n",
    "def hog_images(images, plot=True):\n",
    "    imgs = []\n",
    "    for img in images:\n",
    "        val = hog(img, plot)\n",
    "        imgs.append(val)\n",
    "    \n",
    "    return imgs\n",
    "\n",
    "fd = hog(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "i:  0\n",
      "i:  0\n",
      "i:  0\n",
      "i:  0\n",
      "i:  0\n",
      "i:  0\n",
      "i:  0\n",
      "i:  0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "img_train = []\n",
    "tr_label = []\n",
    "directory = \"../../../MIO-TCD/MIO-TCD-Classification/train\"\n",
    "batch_size = 1000\n",
    "max_w = float('-inf')\n",
    "max_h = float('-inf')\n",
    "for file in os.listdir(directory):\n",
    "    label = file\n",
    "    path = directory + \"/\" + file\n",
    "    dir_files = np.array(os.listdir(path))\n",
    "    indices = np.arange(0, min(1000, len(dir_files)), batch_size)\n",
    "    # np.random.shuffle(dir_files)\n",
    "    for i in indices:\n",
    "        vals = []\n",
    "        for f in dir_files[i:i+batch_size]:\n",
    "            img = cv2.imread(path + \"/\" + f, cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            fd = hog(img, False)\n",
    "            vals.append(np.append(fd, label))\n",
    "        vals = np.array(vals)\n",
    "        print(\"i: \", i)\n",
    "        np.savez_compressed(file+str(i), vals)\n",
    "    \n",
    "img_train = np.array(img_train)\n",
    "np.random.shuffle(img_train)\n",
    "tr_label = np.array(tr_label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.array([1, 2, 3]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-c208dc814159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mvals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-1452b6bafce0>\u001b[0m in \u001b[0;36mhog\u001b[1;34m(img, plot)\u001b[0m\n\u001b[0;32m      7\u001b[0m                         \u001b[0mmultichannel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mfeature_vector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                         visualize=plot)\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m# print(fd)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# print(\"fd.shape: \", fd.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py\u001b[0m in \u001b[0;36mhog\u001b[1;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, visualise, transform_sqrt, feature_vector, multichannel)\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morientation_histogram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[0mnormalized_blocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0m_hog_normalize_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblock_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \"\"\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_train = []\n",
    "tr_label = []\n",
    "directory = \"../../../MIO-TCD/MIO-TCD-Classification/train\"\n",
    "batch_size = 100\n",
    "max_w = float('-inf')\n",
    "max_h = float('-inf')\n",
    "for file in os.listdir(directory):\n",
    "    label = file\n",
    "    path = directory + \"/\" + file\n",
    "    dir_files = np.array(os.listdir(path))\n",
    "    indices = np.arange(0, len(dir_files), batch_size)\n",
    "    for i in indices:\n",
    "        vals = []\n",
    "        for f in dir_files[i:i+batch_size]:\n",
    "            img = cv2.imread(path + \"/\" + f, cv2.IMREAD_COLOR)\n",
    "            img = resizeAndPad(img, 720)\n",
    "            fd = hog(img, False)\n",
    "            vals.append(np.append(fd, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = []\n",
    "tr_label = []\n",
    "directory = \"../../../MIO-TCD/MIO-TCD-Classification/train\"\n",
    "batch_size = 100\n",
    "max_w = float('-inf')\n",
    "max_h = float('-inf')\n",
    "for file in os.listdir(directory):\n",
    "    label = file\n",
    "    path = directory + \"/\" + file\n",
    "    dir_files = np.array(os.listdir(path))\n",
    "    indices = np.arange(0, len(dir_files), batch_size)\n",
    "\n",
    "    for i in indices:\n",
    "        vals = []\n",
    "        for f in dir_files[i:i+batch_size]:\n",
    "            img = cv2.imread(path + \"/\" + f, cv2.IMREAD_COLOR)\n",
    "            img = resizeAndPad(img, 720)\n",
    "            fd = hog(img, False)\n",
    "            vals.append(np.append(fd, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_w)\n",
    "print(max_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('''C:\\\\Users\\\\Ali\\\\Desktop\\\\ECSE 415\\\\Assignments\\\\Project\\\\MIO-TCD\\\\MIO-TCD-Classification\\\\train\\\\bicycle\\\\00002069.jpg''', cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col = 4\n",
    "nbins = 8\n",
    "def plot_all_hog(gradient):\n",
    "    nb_row = math.ceil(nbins / max_col) * 100\n",
    "    nb_plot = nb_row + max_col * 10\n",
    "    plt.figure(figsize = (20,10))\n",
    "    for i in range(nbins):\n",
    "        plt.subplot(nb_plot + i + 1)\n",
    "        plt.pcolor(gradient[:, :, i])\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.title(\"HOG bin = \" + str(i)), plt.xticks([]), plt.yticks([])\n",
    "        plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoG Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 6.57068393e-04, 6.59597907e-04, ...,\n",
       "       9.99982833e-01, 9.99982833e-01, 9.99984090e-01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier()\n",
    "train_vals = hog_train_imgs.reshape(hog_train_imgs.shape[0], -1)\n",
    "neigh.fit(train_vals, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_vals = np.array(hog_images(image_test, plot=True))\n",
    "test_vals = test_vals.reshape(test_vals.shape[0], -1)\n",
    "\n",
    "print(neigh.predict(test_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Reasoning Questions\n",
    "### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HoG is not rotation invariant meaning that if you calculate HoG on an image and calculate HoG on a rotated version of that image, the results will not be the same. For a classification example you could also include rotated images into your training set to make your classifier better recognize them. Another approach could be to use another feature detector such as SIFT which is rotation invariant. One approach to ensure all logos have a uniform orientation is to maintain one image for each class we are trying to classify that we know is oriented in the right direction. Then for every logo that comes in, we would need to calculate the SIFT features, match them between the two images and rotate the images such that they match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)\n",
    "There would be two gradients needed to accurately depict the picture. This is because with only one gradient, you would not be able to distinguish between the left most image and the other two images. With the second gradient direction we would be able to have two reference points and uniquely identify all the images. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
